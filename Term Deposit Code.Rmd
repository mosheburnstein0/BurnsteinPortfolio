---
title: "DSC630 Course Project"
author: "Moshe Burnstein"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Load in bank data
```{r}
bank_df <- read.csv('bank-additional-full.csv', sep=';', header=TRUE, stringsAsFactors = TRUE)
```
Check balance of dataset in yes/no subscribing, the target.
```{r}
library(vctrs)
subscriber_counts <- vec_count(bank_df$y)
subscriber_counts
percent_subscribed <- 4640/36548*100
percent_subscribed
```
The target variable is imbalanced. There are 36,548 who declined to subscribe, and 4,640 who subscribed. Only 12.7% subscribed.

Create new df for subscribers and a separate df for non-subscribers in order to compare distibutions of the different groups.
```{r}
yes_subscribe_df <- bank_df[bank_df$y == 'yes',]
```
Create df of non-subscribers
```{r}
non_subscribe_df <- bank_df[bank_df$y != 'yes',]
```
Visualize the distributions of age in the entire df, in the subscribe df, and in the non-subscribe df.
```{r}
hist(bank_df$age, col = 1)
hist(yes_subscribe_df$age, col = 1)
hist(non_subscribe_df$age, col = 1)
```
One must take note that while the x-axis is on the same scale throughout, aside from no 100 year-old outliers in the non-subscribe df, the scale of the y-axis frequency is up to 1,000, and not 8,000. This is because there are only a fraction of the number of observations in the subscribe df as there are in the entire df, or in the non-subscribe df. The shape of the distribution differs from subscribers to non-subscribers. The vast majority of non-subscribers are between 20 and 60 years of age, while subscribers are well-represented both in the under 20 age bin and in the over 60 bins.

Check balance of subscribers in people younger or equal to 20 and older or equal to 60.
```{r}
old_young_df <- bank_df[bank_df$age <= 20 | bank_df$age >= 60,]
library(vctrs)
old_young_subscribe_counts <- vec_count(old_young_df$y)
old_young_subscribe_counts
subscribe_percent <- 529/804*100
subscribe_percent
```
Yes-subscribers make up 65.8% of the population.

Plot euribor3m distributions for entire population, for yes-subscribers, and for non-subscribers.
```{r}
hist(bank_df$euribor3m, breaks = 100, col = 2)
hist(yes_subscribe_df$euribor3m, breaks = 100, col = 2)
hist(non_subscribe_df$euribor3m, breaks = 100, col = 2)
```
Note that the scales of the three plots are all different ranges because of the differing magnitudes of counts. It is clear, however, that the yes subscibers are right-skewed as opposed to the entire population and the non subscribers. The lower euribor3m rates are positively correlated with subscriptions.

Plot duration
```{r}
hist(bank_df$duration, col = 3)
hist(yes_subscribe_df$duration, col = 3)
hist(non_subscribe_df$duration, col = 3)
```
There are clearly more phone calls of longer duration in yes-subscribe.

Prepare data for modeling by scaling, converting to factor
```{r warning=FALSE}
library(fastDummies)
bank_df_dummies <- dummy_cols(bank_df[,-21])
bank_df_dummies$y <- bank_df$y
bank_df_dummies$y <- as.numeric(bank_df_dummies$y)
x <- sapply(bank_df_dummies, is.factor)
bank_df_dummies[, x] <- as.data.frame(apply(bank_df_dummies[, x], 2, as.numeric))
bank_df_scaled <- scale(bank_df_dummies[, -74])
bank_df_scaled$y <- bank_df_dummies$y
bank_df_dummies <- bank_df_dummies[, colSums(is.na(bank_df_dummies))==0]
bank_df_dummies$y <- as.factor(bank_df_dummies$y)
```
Create 70/30 train/test split. Use 'sample.split' from the caTools library.
```{r}
library(caTools)
split <- sample.split(bank_df_dummies, SplitRatio = 0.7)
train <- subset(bank_df_dummies, split == "TRUE")
test <- subset(bank_df_dummies, split == "FALSE")
```
Try logistic regression (GLM) on entire dataset
```{r warning=FALSE}
set.seed(334)
logit_model <- glm(y ~.,family=binomial(link='logit'),data=train)
anova(logit_model, test = 'Chisq')
library(pscl)
pR2(logit_model)
```
```{r warning=FALSE}
library(ROCR)
p <- predict(logit_model,newdata=test,type='response')
pr <- prediction(p, test$y)
prf <- performance(pr, measure = 'tpr', x.measure = 'fpr')
plot(prf)
```
```{r}
auc <- performance(pr, measure = 'auc')
auc <- auc@y.values[[1]]
auc
```
```{r}
library(caret)
head(p, 10)
p.rd <- ifelse(p > 0.5, 1, 0)
head(p.rd, 10)
confusionMatrix(table(p.rd,test[,18]))
```
Note the utter failure in specificity(0.03686). Not a useful model.

Try glm with only duration and emp.var.rate
```{r}
library(dplyr)
bank_df_subset <- bank_df %>% select(duration, emp.var.rate, y)
bank_df_subset_scaled <- scale(bank_df_subset[, -3])
bank_df_subset_scaled <- as.data.frame(bank_df_subset_scaled)
bank_df_subset_scaled$y <- bank_df_subset$y
```
Splitting data in train and test data
```{r}
train <- bank_df_subset_scaled[1:30000,]
test <-  bank_df_subset_scaled[30001:41188,]
```
Create GLM 
```{r}
set.seed(221)
logit_model <- glm(y ~.,family=binomial(link='logit'),data=train)
anova(logit_model, test = 'Chisq')
```
Must resort to pseudo-Rsquared instead of Rsquared because this is a classification problem.
```{r}
library(pscl)
pR2(logit_model)
```
```{r}
library(ROCR)
p <- predict(logit_model,newdata=test,type='response')
pr <- prediction(p, test$y)
prf <- performance(pr, measure = 'tpr', x.measure = 'fpr')
plot(prf)
```
```{r}
auc <- performance(pr, measure = 'auc')
auc <- auc@y.values[[1]]
auc
```
The subset model is not as robust as the GLM on the entire dataset

Try RandomForest. No need to scale because it is tree-based
Fitting Random Forest to the train dataset
```{r}
bank_df_dummies <- bank_df_dummies[, colSums(is.na(bank_df_dummies))==0]
bank_df_dummies$y <- as.factor(bank_df_dummies$y)
```
Split data train/test
```{r}
library(caTools)
split <- sample.split(bank_df_dummies, SplitRatio = 0.7)
train <- subset(bank_df_dummies, split == "TRUE")
test <- subset(bank_df_dummies, split == "FALSE")
```
```{r}
library(randomForest)
set.seed(126) # Setting seed
classifier_RF = randomForest(x = train[-64],
                             y = train$y,
                             keep.forest = TRUE,
                             ntree = 400)
```
Variable Importance Plot
```{r fig.height=8, fig.width=10}
varImpPlot(classifier_RF)
```
Display the decision trees
```{r fig.height=16, fig.width=18}
library(party)
x <- ctree(y ~ ., data = bank_df_dummies)
plot(x, type = 'simple')
```
Accuracy on test set
```{r}
accuracy = (24469 + 1342)/(24469 + 1342 + 1836 + 669)
accuracy
```
Score model on test set
```{r}
library(caret)
y_predict <- predict(classifier_RF, test)
confusionMatrix(y_predict, test$y)
```
More robust than logistic regression, but specificity well below 50%.

Fit PCR Model to visualize important components. Calculate how much we can minimize 
features to model.
```{r}
library(ISLR)
library(pls)
set.seed(2)
bank_df_dummies$y <- as.numeric(bank_df_dummies$y)
pcr.fit=pcr(bank_df_dummies$y~., data=bank_df_dummies, scale=TRUE,
            validation ="CV")
```
Visualize cross-validation plots
```{r}
validationplot(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
```
One must use upwards of 37 components to create a viable model. 
Calculate log loss


Try kNN. k = 60
```{r}
library(caret)
library(class)
set.seed(234)
knn_60 <- knn(train=train[-64], test=test[-64], cl=train$y, k=60)
ACC.60 <- 100 * sum(test$y == knn_60)/NROW(test$y)
ACC.60
table(knn_60, test$y)
```
```{r}
confusionMatrix(table(knn_60, test$y))
```
The best kNN does not produce 50% specificity.

Create SVM Model
```{r}
# Create the classifier here
# install.packages("e1071")
# you can also use kernlab
library(e1071)
classifier <- svm(formula = y ~ .,
                  data = train,
                  type = "C-classification",
                  kernel = "radial", 
                  cost = 0.25, 
                  cross = 10,
                  sigma = 1.22723
) 
y_pred <-  predict(classifier, newdata=test[-64])
```
```{r}
library(caret)
confusionMatrix(data = (y_pred),       
                reference = test[,64])

```
This SVM Model failed miserably in predicting subscribers, 1125 false positives,
and only 350 true positives. 

Try bagged model
```{r fig.height=18, fig.width=10}
library(dplyr)       #for data wrangling
library(e1071)       #for calculating variable importance
library(caret)       #for general model fitting
library(rpart)       #for fitting decision trees
library(ipred)       #for fitting bagged decision trees
set.seed(1)

#fit the bagged model
bag <- bagging(
  formula = y ~ .,
  data = train,
  nbagg = 150,   
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

#display fitted bagged model
bag
pred <- predict(object = bag, 
                newdata = test,  
                type = "class") 

```
```{r}
library(caret)
confusionMatrix(data = as.factor(pred),       
                reference = as.factor(test$y)) 
```

Try xgboost model

```{r}
library(gbm)
library(caret)
model_gbm = gbm(y ~.,
                data = train,
                distribution = "multinomial",
                cv.folds = 10,
                shrinkage = .01,
                n.minobsinnode = 10,
                n.trees = 500)       # 500 trees to be built
summary(model_gbm)
pred_test = predict.gbm(object = model_gbm,
                        newdata = test,
                        n.trees = 500,           # 500 trees to be built
                        type = "response")
pred_test
# Give class names to the highest prediction value.
class_names = colnames(pred_test)[apply(pred_test, 1, which.max)]
result = data.frame(test$y, class_names)
print(result)
conf_mat = confusionMatrix(test$y, as.factor(class_names))
print(conf_mat)

```
This is the most robust model with a specificity of 0.7320.





















